# -*- coding: utf-8 -*-
"""YelpReview_Processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z2iFRqB8BcsUxP7P4uZsTRdjRX22vcM_
"""

!pip install pyspark
from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
from pyspark import SparkConf

#from pyspark.ml.feature import Tokenizer, HashingTF, IDF
#from pyspark.ml.classification import LogisticRegression
#from pyspark.ml import Pipeline
from pyspark.sql.functions import col

conf = SparkConf().setMaster("local[*]").set("spark.executer.memory", "10g")

sc = SparkContext(conf=conf)
spark = SparkSession(sc).builder.getOrCreate()
spark.sparkContext.setLogLevel("ERROR")

df = spark.read.format("csv").option("header", "true").option("multiline","true").load("/content/drive/MyDrive/yelp-dataset/yelp_review.csv")
df.printSchema()

df = df.withColumn("review_stars", df["stars"].cast("double"))
df = df.filter(df.review_stars.isin(4.0,5.0))
df = df.drop("stars")
df = df.drop("useful","funny","cool")

yelp_business_df = spark.read.csv('/content/drive/MyDrive/yelp-dataset/yelp_business.csv', header=True, inferSchema=True)
yelp_business_df.printSchema()
yelp_business_df = yelp_business_df.drop("neighborhood","address","city","postal_code","latitude","longitude")
yelp_business_df = yelp_business_df.withColumnRenamed("stars", "business_stars")
#yelp_business_df.show(60)

yelp_business_df = yelp_business_df.filter(yelp_business_df["is_open"] == 1.0)

food_words = ['Restaurant', 'Restaurants', 'Food', 'Fast Food', 'Bakery', 'Ice Cream & Frozen Yogurt', 'Cafe', 'Soup', 'Salad', 'Coffee', 'Tea', 'Food Truck', 'Pizza', 'Sandwich', 'Bar', 'Buffet', 'Diner', 'Donut', 'Juice Bar', 'Smoothie', 'Deli', 'Bagel', 'Dessert', 'Food Stand', 'Street Vendor', 'Tea Room']
cuisine = ["American", "Italian", "Mexican", "Chinese", "Japanese", "Indian", "French", "Thai", "Mediterranean", "Greek", "Spanish", "Vietnamese", "Korean", "Lebanese", "Brazilian", "Turkish", "Caribbean", "Ethiopian"]

from pyspark.sql.functions import split, explode, col, collect_list


tokenized_df = yelp_business_df.withColumn("category_tokens", explode(split(col("categories"), ";")))

# Apply the filtering logic to include only rows where category_tokens match food_words or cuisine
filtered_df = tokenized_df.filter(
    (tokenized_df["category_tokens"].isin(food_words)) |
    (tokenized_df["category_tokens"].isin(cuisine))
)

# Group by business_id and aggregate category tokens into a list
grouped_df = filtered_df.groupBy("business_id").agg(collect_list("category_tokens").alias("category_tokens"))

# Join the original DataFrame with the grouped DataFrame on business_id
result_df = yelp_business_df.join(grouped_df, "business_id", "inner")

reviews_df = df.drop("user_id", "date")

from pyspark.sql.types import DoubleType

# Convert the "business_stars" column to DoubleType
result_df = result_df.withColumn("business_stars", result_df["business_stars"].cast(DoubleType()))

# Filter the DataFrame to include only rows where "business_stars" is above 3.49
above_average_df = result_df.filter(result_df["business_stars"] > 3.49)

merged_df = above_average_df.join(reviews_df, "business_id", "inner")

from pyspark.sql.functions import desc

# Group by the "state" column and count occurrences
state_counts_df = merged_df.groupBy("state").count()

# Sort the DataFrame by count in descending order
sorted_state_counts_df = state_counts_df.orderBy(desc("count"))

# Select the top 5 most occurring values
top_5_states = sorted_state_counts_df.limit(5)

# Show the top 5 most occurring values in the "state" column with their counts
top_5_states.show()

from pyspark.sql.window import Window
from pyspark.sql.functions import rank, col

# Define the list of states
states_list = ['NV', 'AZ', 'ON', 'OH', 'PA']

# Filter the DataFrame to include only the specified states
filtered_df = merged_df.filter(merged_df["state"].isin(states_list))

# Define a window partitioned by state and ordered by review_count descending,
# and then by name ascending to break ties
windowSpec = Window.partitionBy("state").orderBy(col("review_count").desc(), col("name"))

# Add a rank column to each row within each state, ordered by review_count descending and name ascending
ranked_df = filtered_df.withColumn("rank", rank().over(windowSpec))

# Filter the DataFrame to include only the top 5 ranked businesses for each state
top_businesses_df = ranked_df.filter(ranked_df["rank"] <= 5)

# Select the required columns
result_df = top_businesses_df.select("state", "name", "review_count").distinct()

# Show the resulting DataFrame
result_df.show()


from pyspark.sql.functions import split, explode, col

# Filter the DataFrame to include only the specified states and cuisines
filtered_df = merged_df.filter((col("state").isin(states_list)) & (col("categories").rlike("|".join(cuisine_list))))

# Split the categories string into an array
filtered_df = filtered_df.withColumn("category_array", split(col("categories"), ";"))

# Explode the category_array column to get a row for each category
exploded_df = filtered_df.withColumn("cuisine", explode(col("category_array")))

# Group by state and cuisine, and count occurrences
result_df = exploded_df.groupBy("state", "cuisine").count().orderBy("state", col("count").desc())

result_df = result_df.withColumnRenamed("count", "review_count")
# Show the resulting DataFrame
result_df.show()
#result_df.count(), count = 1063

from pyspark.sql.functions import max

cuisine_list = ["American", "Italian", "Mexican", "Chinese", "Japanese", "Indian", "French", "Thai", "Mediterranean", "Greek", "Spanish", "Vietnamese", "Korean", "Lebanese", "Brazilian", "Turkish", "Caribbean", "Ethiopian"]

# Define your states list
states_list = ['NV', 'AZ', 'ON', 'OH', 'PA']

# Filter the DataFrame for cuisines present in cuisine_list
filtered_df = result_df.filter(result_df['cuisine'].isin(cuisine_list))

# Iterate over each state in states_list
for state in states_list:
    # Filter DataFrame for the current state
    state_df = filtered_df.filter(filtered_df['state'] == state)

    # Find the cuisine with maximum review_count for each state
    max_review_count_df = state_df.groupBy('state').agg(max('review_count').alias('max_review_count'))

    # Join with the original DataFrame to get additional cuisine details
    result = state_df.join(max_review_count_df, 'state', 'inner').select('state', 'cuisine', 'review_count').filter(state_df['review_count'] == max_review_count_df['max_review_count'])

    # Show the result
    print(f"State: {state}")
    result.show(truncate=False)




from pyspark.sql.functions import col,count, desc

# Group by 'state' and count the number of reviews for each state
state_review_count = merged_df.groupBy('state').agg(count('review_id').alias('review_count'))
# Order by review_count in descending order
state_review_count = state_review_count.orderBy(desc('review_count'))
# Show the top 5 states with the most amount of reviews
top_5_states = state_review_count.limit(6)

top_5_states_list = top_5_states.select('state').rdd.flatMap(lambda x: x).collect()

# Print the top 5 states with the most amount of reviews
print("Top 5 States with the Most Amount of Reviews:")
for state in top_5_states_list:
    print(state)

top_5_states_list = top_5_states.select('state').rdd.flatMap(lambda x: x).collect()

# Filter the merged_df DataFrame to include only the top 5 states
top_5_states_df = merged_df.filter(col('state').isin(top_5_states_list))

# Group by 'business_id' and count the number of reviews for each business
business_review_count = top_5_states_df.groupBy('business_id', 'name').agg(count('review_id').alias('review_count'))

# Order by review_count in descending order
business_review_count = business_review_count.orderBy(desc('review_count'))

# Show the top 5 business IDs and names of those 5 businesses
top_5_businesses = business_review_count.limit(5)
top_5_businesses.show()


from pyspark.sql.functions import split, explode, col

from pyspark.sql.functions import col, count, desc, first, length, regexp_extract

# Filter the DataFrame to include only the specified states and cuisines
filtered_df = merged_df.filter((col("state").isin(states_list)) & (col("categories").rlike("|".join(cuisine_list))))


# Filter the DataFrame to include only states with exactly two letters and no numbers
merged_df_filtered = filtered_df.filter(length(col('state')) == 2) \
                              .filter(regexp_extract(col('state'), '^[A-Za-z]+$', 0) != '')

# Group by 'state' and count the number of reviews for each state
state_review_count = merged_df_filtered.groupBy('state').agg(count('review_id').alias('review_count'))
# Order by review_count in descending order
state_review_count = state_review_count.orderBy(desc('review_count'))
# Show the top 5 states with the most amount of reviews
top_5_states = state_review_count.limit(5)

top_5_states_list = top_5_states.select('state').rdd.flatMap(lambda x: x).collect()

# Print the top 5 states with the most amount of reviews:
print("Top 5 States with the Most Amount of Reviews:")
for state in top_5_states_list:
    print(state)

# Loop through top 5 states and find the business with the most reviews for each state
for state in top_5_states_list:
    print("\nState:", state)

    # Filter the DataFrame for the current state
    state_df = merged_df_filtered.filter(col('state') == state)

    # Group by 'business_id' and count the number of reviews for each business
    business_review_count = state_df.groupBy('business_id', 'name').agg(count('review_id').alias('review_count'), first('business_stars').alias('business_stars'))

    # Order by review_count in descending order
    business_review_count = business_review_count.orderBy(desc('review_count'))

    #business_review_count.printSchema()

    # Get the business with the most reviews for the current state
    top_business = business_review_count.first()
    if top_business:
        print("Business with the most reviews:", top_business['name'], "(Business ID:", top_business['business_id'] + ")")
        print("Business Stars:", top_business['business_stars'])
        print("Business Review Count", top_business['review_count'])

    else:
        print("No businesses found for this state.")


##### State and Review Stars#######
#most five star reviewed resturant per state

# Filter the DataFrame to include only the specified states
filtered_df = merged_df.filter(merged_df["state"].isin(states_list))

# Define a window partitioned by state and ordered by stars descending,
# and then by review_count descending and name ascending to break ties
windowSpec = Window.partitionBy("state").orderBy(col("review_stars").desc(), col("review_stars").desc(), col("name"))

# Add a rank column to each row within each state, ordered by stars descending, review_count descending, and name ascending
ranked_df = filtered_df.withColumn("rank", rank().over(windowSpec))

# Filter the DataFrame to include only the top-ranked businesses for each state
top_businesses_df = ranked_df.filter(ranked_df["rank"] == 1)

# Select the required columns
result_df = top_businesses_df.select("state", "name", "review_stars").distinct()

# Show the resulting DataFrame
result_df.show()

from pyspark.sql.functions import col, avg, sum, array_contains, collect_set, when, explode, split
import folium
import matplotlib.pyplot as plt
import pandas as pd
from math import pi
from pyspark.sql import functions as F

states_of_interest = ['AZ', 'NV', 'OH', 'PA']
filtered_states_df = merged_df.filter(col("state").isin(states_of_interest))

# Exploding the categories into separate rows for each category associated with each restaurant
exploded_cuisines_df = filtered_states_df.withColumn("cuisine", explode(split(col("categories"), ";")))

exploded_cuisines_df.select("business_id", "name", "state", "cuisine").show(truncate=False)

# Aggregate data by state and cuisine to find average ratings and count of reviews
cuisine_ratings = exploded_cuisines_df.groupBy("state", "cuisine").agg(
    avg("business_stars").alias("average_rating"),
    count("business_id").alias("restaurant_count")
).orderBy("state", "average_rating", ascending=False)

# Show the aggregated data
cuisine_ratings.show(truncate=False)

# List of restaurant keywords to remove all non food related categories in the cuisine list
food_keywords = ['Cafe', 'Pizza', 'Bakery', 'Bistro', 'Bar', 'Grill', 'Steakhouse', 'Italian', 'Mexican', 'Chinese', 'Japanese', 'Seafood', 'Sushi', 'Indian', 'Thai', 'Korean', 'Vietnamese', 'French', 'Mediterranean', 'Lebanese', 'Turkish', 'Greek', 'Asian', 'Fusion']

# Filter dataframe to only contain the food_keywords
filtered_cuisines_df = exploded_cuisines_df.filter(
    exploded_cuisines_df.cuisine.isin(food_keywords)
)

# Aggregate filtered data
cuisine_ratings = filtered_cuisines_df.groupBy("state", "cuisine").agg(
    avg("business_stars").alias("average_rating"),
    count("business_id").alias("restaurant_count")
).orderBy("state", "average_rating", ascending=False)

cuisine_ratings.show(truncate=False)

# defining the states in each region
western_states = ['AZ', 'NV']
eastern_states = ['OH', 'PA']

# adding 'region' column to the df
filtered_cuisines_df = filtered_cuisines_df.withColumn(
    'region',
    when(col("state").isin(western_states), "West")
    .when(col("state").isin(eastern_states), "East")
    .otherwise("Other")
)

# aggregating data by region and cuisine
region_cuisine_ratings = filtered_cuisines_df.groupBy("region", "cuisine").agg(
    avg("business_stars").alias("average_rating"),
    count("business_id").alias("restaurant_count")
).orderBy("region", "average_rating", ascending=False)

significant_cuisines = region_cuisine_ratings.filter(col("restaurant_count") > 50)  # limit to restaurant counts of atleast 50

significant_cuisines.show(50, truncate=False)

# obtaining coordinates for each restaurant
cuisine_location_data = filtered_cuisines_df.join(
    significant_cuisines,
    ["cuisine", "region"],
    "inner"
).select("name", "cuisine", "region", "latitude", "longitude", "average_rating", "restaurant_count")

# filtered for visualization
cuisine_location_data = cuisine_location_data.filter((col("latitude").isNotNull()) & (col("longitude").isNotNull()))

significant_cuisines_pd = significant_cuisines.toPandas()

pivot_df = significant_cuisines_pd.pivot(index='cuisine', columns='region', values='average_rating')

# sum east and west ratings to get total
pivot_df['total_rating'] = pivot_df.sum(axis=1)

# sort by total_rating
pivot_df.sort_values(by='total_rating', ascending=True, inplace=True)

# drop total rating column for plotting
pivot_df.drop(columns=['total_rating'], inplace=True)

fig, ax = plt.subplots(figsize=(15, 7))
pivot_df.plot(kind='bar', ax=ax, color=['green', 'orange'])
ax.grid(True, linestyle='--', which='both', color='grey', alpha=0.5)

ax.set_title('East vs West Cuisine Ratings Sorted by Total Ratings')
ax.set_xlabel('Cuisine')
ax.set_ylabel('Average Rating')
ax.set_xticklabels(pivot_df.index, rotation=45)
ax.legend(title='Region', loc='lower left')

plt.tight_layout()
plt.show()

def find_best_region_for_cuisine(cuisine_name):
    # filter df for cuisines
    cuisine_data = significant_cuisines.filter(F.col("cuisine") == cuisine_name)

    # pivot df to get average rating by region
    pivot_data = cuisine_data.groupBy("cuisine").pivot("region", ["East", "West"]).agg(F.first("average_rating"))

    # collect data into local dictionary
    ratings = pivot_data.collect()[0].asDict()

    # checking both regions for the higher rating
    if ratings.get('East', 0) > ratings.get('West', 0):
        return "East"
    elif ratings.get('East', 0) < ratings.get('West', 0):
        return "West"
    else:
        return "Tie or insufficient data"

# change cuisine_question to find recommendation
cuisine_question = "Korean"
best_region = find_best_region_for_cuisine(cuisine_question)
print(f"The best region for {cuisine_question} food is: {best_region}")
from pyspark.sql.functions import split, explode

# splitting and exploding the categories column to treat each category as a potential cuisine
exploded_df = merged_df.withColumn("cuisine", explode(split(col("categories"), ";")))
filtered_cuisines_df = exploded_df.filter(col("cuisine").isin(food_keywords))
filtered_cuisines_df.show()

cuisines = significant_cuisines_pd['cuisine'].unique()
east_reviews = significant_cuisines_pd[significant_cuisines_pd['region'] == 'East'].set_index('cuisine')['restaurant_count']
west_reviews = significant_cuisines_pd[significant_cuisines_pd['region'] == 'West'].set_index('cuisine')['restaurant_count']

# Create the bar chart
fig, ax = plt.subplots(figsize=(18, 8))
bar_width = 0.40
index = range(len(cuisines))

bars1 = ax.bar(index, east_reviews.reindex(cuisines).fillna(0), bar_width, label='East', color='green')
bars2 = ax.bar([i + bar_width for i in index], west_reviews.reindex(cuisines).fillna(0), bar_width, label='West', color='orange')

# Adding the average rating on top of each bar
for bar in bars1 + bars2:
    height = bar.get_height()
    ax.annotate(f'{height:.0f}',
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3),  # 3 points vertical offset
                textcoords="offset points",
                ha='center', va='bottom')

# Customize the plot
ax.set_xlabel('Cuisine')
ax.set_ylabel('Review Count')
ax.set_title('Comparison of Review Counts for Cuisines in East vs West')
ax.set_xticks([i + bar_width / 2 for i in index])
ax.set_xticklabels(cuisines, rotation=45)
ax.legend()

plt.tight_layout()
plt.show()

# manually defining color scheme for each cuisine
cuisine_colors = {
    'Korean': 'darkcyan',
    'Greek': 'c',
    'Mediterranean': 'cyan',
    'French': 'darkturquoise',
    'Lebanese': 'powderblue',
    'Indian': 'deepskyblue',
    'Turkish': 'skyblue',
    'Japanese': 'lightskyblue',
    'Mexican': 'steelblue',
    'Pizza': 'aliceblue',
    'Italian': 'dodgerblue',
    'Thai': 'aquamarine',
    'Seafood': 'slategrey',
    'Vietnamese': 'lightcyan',
    'Chinese': 'mediumspringgreen'
}

# function to get colors from cuisine
def get_colors(cuisines, cuisine_colors):
    return [cuisine_colors.get(cuisine, 'gray') for cuisine in cuisines]

# generate colors for each slice
east_colors = get_colors(east_reviews.index, cuisine_colors)
west_colors = get_colors(west_reviews.index, cuisine_colors)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))

# East
ax1.pie(east_percentages, labels=east_reviews.index, autopct='%1.1f%%', startangle=140, colors=east_colors)
ax1.axis('equal')
ax1.set_title('East Region Cuisine Distribution')

# West
ax2.pie(west_percentages, labels=west_reviews.index, autopct='%1.1f%%', startangle=140, colors=west_colors)
ax2.axis('equal')
ax2.set_title('West Region Cuisine Distribution')

plt.tight_layout()
plt.show()

cuisine = ["American", "Italian", "Mexican", "Chinese", "Japanese", "Indian", "French", "Thai", "Mediterranean", "Greek", "Spanish", "Vietnamese", "Korean", "Lebanese", "Brazilian", "Turkish", "Caribbean", "Ethiopian"]

# merged_df2 = merged_df.withColumn("cuisine_list", sqlf.lit(cuisine))
merged_df2 = merged_df.filter(sqlf.arrays_overlap(merged_df['category_tokens'],sqlf.lit(cuisine)))
# merged_df2.show()
merged_df2.persist()

import string
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk import punkt
stopword_list = set(stopwords.words('english'))
stopword_list.update(['food', 'great', 'good', 'place','amazing', 'restaurant', 'back', 'delicious', 'really', 'best', 'love', 'go', 'like', 'get'])

def ProcessText(text):
  tokens = nltk.word_tokenize(text)
  remove_punct = [word.lower() for word in tokens if word.isalpha()]
  remove_stop_words = [word for word in remove_punct if not word in stopword_list]
  return remove_stop_words

output = []
num_topics = 3
iter_num = 10
min_doc_freq = 0.05
max_doc_freq = 0.90

cuisine = ["American", "Italian", "Mexican", "Chinese", "Japanese", "Indian", "French", "Thai", "Mediterranean", "Greek", "Spanish", "Vietnamese", "Korean", "Lebanese", "Brazilian", "Turkish", "Caribbean", "Ethiopian"]
#cuisine = ["Italian", "Mexican", "Chinese"]

for key_cuisine in cuisine:
  # First filter merged_df2 to only the cuisine we care about
  cuisine_df = merged_df2.filter(sqlf.arrays_overlap(merged_df2["category_tokens"], sqlf.lit([key_cuisine])))\
    .select("business_id", "review_count", "review_id", "text")\
    .withColumn("cuisine", sqlf.lit(key_cuisine)
    )

  if cuisine_df.count() == 0: continue

  # Tokenizing the text of the filtered reviews
  tok_rev = cuisine_df.rdd.map(lambda x: [x['business_id'], x['review_count'], x['review_id'], ProcessText(x['text']), x['cuisine']]).toDF()
  tok_rev = tok_rev.withColumnsRenamed(dict(zip(tok_rev.columns, cuisine_df.columns)))

  # Count vectorizing the text of the reviews
  cv = CountVectorizer(minDF=min_doc_freq, maxDF=max_doc_freq, inputCol='text', outputCol = 'features')
  cv_model = cv.fit(tok_rev)
  cv_rev = cv_model.transform(tok_rev)

  # Initializing the LDA model and fitting it to count vectorized reviews
  lda = LDA(k=num_topics, maxIter=iter_num)
  model = lda.fit(cv_rev)

  # Extracting the words from the topics of the LDA model
  vocab = cv_model.vocabulary
  topics = model.describeTopics()

  topic_groups = topics.rdd.map(lambda row: (key_cuisine, row['topic'], row['termIndices']))\
    .map(lambda idx_list: (idx_list[0], idx_list[1], [vocab[idx] for idx in idx_list[2]]))\
    .collect()

  # Extending the output list
  output.extend(topic_groups)
  print(key_cuisine, 'analysis completed')

rev_topics = spark.createDataFrame(output,['cuisine','topic','topic_words'])

rev_topics.write.csv('review_topics')

####### Alternate code to look at topics for Japanese cuisine by state ########

output = []
num_topics = 3
iter_num = 10
min_doc_freq = 0.05
max_doc_freq = 0.90

# cuisine = ["American", "Italian", "Mexican", "Chinese", "Japanese", "Indian", "French", "Thai", "Mediterranean", "Greek", "Spanish", "Vietnamese", "Korean", "Lebanese", "Brazilian", "Turkish", "Caribbean", "Ethiopian"]
cuisine = ["Italian", "Mexican", "Chinese"]
key_cuisine = 'Japanese'
states = ['AZ', 'PA']

# for key_cuisine in cuisine:
for state in states:

  # Filtering for the state
  cuisine_df = merged_df2.filter(sqlf.arrays_overlap(merged_df2["category_tokens"], sqlf.lit([key_cuisine])))\
    .filter(merged_df2["state"] == state)\
    .select("business_id", "state", "review_count", "review_id", "text")\
    .withColumn("cuisine", sqlf.lit(key_cuisine)
    )

  print(state,"rows:",cuisine_df.count())
  if cuisine_df.count() == 0: continue

  # Tokenizing the text of the filtered reviews
  tok_rev = cuisine_df.rdd.map(lambda x: [x['business_id'], x['state'], x['review_count'], x['review_id'], ProcessText(x['text']), x['cuisine']]).toDF()
  tok_rev = tok_rev.withColumnsRenamed(dict(zip(tok_rev.columns, cuisine_df.columns)))

  # Count vectorizing the text of the reviews
  cv = CountVectorizer(minDF=min_doc_freq, maxDF=max_doc_freq, inputCol='text', outputCol = 'features')
  cv_model = cv.fit(tok_rev)
  cv_rev = cv_model.transform(tok_rev)

  # Initializing the LDA model and fitting it to count vectorized reviews
  lda = LDA(k=num_topics, maxIter=iter_num)
  model = lda.fit(cv_rev)

  # Extracting the words from the topics of the LDA model
  vocab = cv_model.vocabulary
  topics = model.describeTopics()

  topic_groups = topics.rdd.map(lambda row: (state, row['topic'], row['termIndices']))\
    .map(lambda idx_list: (idx_list[0], idx_list[1], [vocab[idx] for idx in idx_list[2]]))\
    .collect()

  # Extending the output list
  output.extend(topic_groups)
  print(state, 'analysis completed')

spark.createDataFrame(output,['state','topic','topic_words']).show(truncate=False)

# Negative Review LDA with Word Clouds

!pip install pyspark

from google.colab import drive
drive.mount('/content/drive')

from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
from pyspark import SparkConf

from pyspark.ml.feature import Tokenizer, CountVectorizer
from pyspark.ml.clustering import LDA
from pyspark.ml import Pipeline
import pyspark.sql.functions as sqlf
from pyspark.sql.types import StructType, StructField, StringType, ArrayType
from pyspark.sql import Row

conf = SparkConf().setMaster("local[*]").set("spark.executer.memory", "10g")

sc = SparkContext(conf=conf)
spark = SparkSession(sc).builder.getOrCreate()
spark.sparkContext.setLogLevel("ERROR")

df = spark.read.format("csv").option("header", "true").option("multiline","true").load("/content/drive/MyDrive/yelp-dataset/yelp_review.csv")
# df.printSchema()

df = df.withColumn("review_stars", df["stars"].cast("double"))
df = df.filter(df.review_stars.isin(1.0,2.0,3.0))
df = df.drop("stars")
df = df.drop("useful","funny","cool")

df.show()

yelp_business_df = spark.read.csv('/content/drive/MyDrive/yelp-dataset/yelp_business.csv', header=True, inferSchema=True)
# yelp_business_df.printSchema()
yelp_business_df = yelp_business_df.drop("neighborhood","address","city","postal_code","latitude","longitude")
yelp_business_df = yelp_business_df.withColumnRenamed("stars", "business_stars")
# yelp_business_df.show(60)

yelp_business_df = yelp_business_df.filter(yelp_business_df["is_open"] == 1.0)
# yelp_business_df.show()

food_words = ['Restaurant', 'Restaurants', 'Food', 'Fast Food', 'Bakery', 'Ice Cream & Frozen Yogurt', 'Cafe', 'Soup', 'Salad', 'Coffee', 'Tea', 'Food Truck', 'Pizza', 'Sandwich', 'Bar', 'Buffet', 'Diner', 'Donut', 'Juice Bar', 'Smoothie', 'Deli', 'Bagel', 'Dessert', 'Food Stand', 'Street Vendor', 'Tea Room']
cuisine = ["American", "Italian", "Mexican", "Chinese", "Japanese", "Indian", "French", "Thai", "Mediterranean", "Greek", "Spanish", "Vietnamese", "Korean", "Lebanese", "Brazilian", "Turkish", "Caribbean", "Ethiopian"]

yelp_business_df.select("categories").show(10,truncate = False)

# tokenized_business_df = yelp_business_df.withColumn("category_tokens", explode(split(col("categories"), ";")))

#  # Filter the DataFrame to include only rows where the category token matches food_words or cuisine
#  filtered_df = tokenized_business_df.filter(
#      (tokenized_business_df["category_tokens"].isin(food_words)) |
#      (tokenized_business_df["category_tokens"].isin(cuisine))
#  )

from pyspark.sql.functions import split, explode, col, collect_list


tokenized_df = yelp_business_df.withColumn("category_tokens", explode(split(col("categories"), ";")))

# Apply the filtering logic to include only rows where category_tokens match food_words or cuisine
filtered_df = tokenized_df.filter(
    (tokenized_df["category_tokens"].isin(food_words)) |
    (tokenized_df["category_tokens"].isin(cuisine))
)

# Group by business_id and aggregate category tokens into a list
grouped_df = filtered_df.groupBy("business_id").agg(collect_list("category_tokens").alias("category_tokens"))

# Join the original DataFrame with the grouped DataFrame on business_id
result_df = yelp_business_df.join(grouped_df, "business_id", "inner")

result_df.show(30)

reviews_df = df.drop("user_id", "date")
# reviews_df.show()

result_df.printSchema()

from pyspark.sql.types import DoubleType

# Convert the "business_stars" column to DoubleType
result_df = result_df.withColumn("business_stars", result_df["business_stars"].cast(DoubleType()))

# Filter the DataFrame to include only rows where "business_stars" is less than or equal to 3.49
below_average_df = result_df.filter(result_df["business_stars"] <= 3.49)

below_average_df.show()

merged_df = below_average_df.join(reviews_df, "business_id", "inner")

merged_df.show()

"""Part II: Analysis"""

# merged_df2 = merged_df.withColumn("cuisine_list", sqlf.lit(cuisine))
merged_df2 = merged_df.filter(sqlf.arrays_overlap(merged_df['category_tokens'],sqlf.lit(cuisine)))
# merged_df2.show()
merged_df2.persist()

import string
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk import punkt
stopword_list = set(stopwords.words('english'))
stopword_list.update(['food', 'great', 'good', 'place','amazing', 'restaurant', 'back', 'delicious', 'really', 'best', 'love', 'go', 'like', 'get'])

def ProcessText(text):
  tokens = nltk.word_tokenize(text)
  remove_punct = [word.lower() for word in tokens if word.isalpha()]
  remove_stop_words = [word for word in remove_punct if not word in stopword_list]
  return remove_stop_words

output = []
num_topics = 3
iter_num = 10
min_doc_freq = 0.05
max_doc_freq = 0.90

cuisine = ["American", "Italian", "Mexican", "Chinese", "Japanese", "Indian", "French", "Thai", "Mediterranean", "Greek", "Spanish", "Vietnamese", "Korean", "Lebanese", "Brazilian", "Turkish", "Caribbean", "Ethiopian"]
# cuisine = ["Italian", "Mexican", "Chinese"]

for key_cuisine in cuisine:
  # First filter merged_df2 to only the cuisine we care about
  cuisine_df = merged_df2.filter(sqlf.arrays_overlap(merged_df2["category_tokens"], sqlf.lit([key_cuisine])))\
    .select("business_id", "review_count", "review_id", "text")\
    .withColumn("cuisine", sqlf.lit(key_cuisine)
    )

  print(key_cuisine,"rows:",cuisine_df.count())
  if cuisine_df.count() == 0: continue

  # Tokenizing the text of the filtered reviews
  tok_rev = cuisine_df.rdd.map(lambda x: [x['business_id'], x['review_count'], x['review_id'], ProcessText(x['text']), x['cuisine']]).toDF()
  tok_rev = tok_rev.withColumnsRenamed(dict(zip(tok_rev.columns, cuisine_df.columns)))

  # Count vectorizing the text of the reviews
  cv = CountVectorizer(minDF=min_doc_freq, maxDF=max_doc_freq, inputCol='text', outputCol = 'features')
  cv_model = cv.fit(tok_rev)
  cv_rev = cv_model.transform(tok_rev)

  # Initializing the LDA model and fitting it to count vectorized reviews
  lda = LDA(k=num_topics, maxIter=iter_num)
  model = lda.fit(cv_rev)

  # Extracting the words from the topics of the LDA model
  vocab = cv_model.vocabulary
  topics = model.describeTopics()

  topic_groups = topics.rdd.map(lambda row: (key_cuisine, row['topic'], row['termIndices']))\
    .map(lambda idx_list: (idx_list[0], idx_list[1], [vocab[idx] for idx in idx_list[2]]))\
    .collect()

  # Extending the output list
  output.extend(topic_groups)
  print(key_cuisine, 'analysis completed')

spark.createDataFrame(output,['cuisine','topic','topic_words']).show(truncate=False)

spark.createDataFrame(output,['cuisine','topic','topic_words']).collect()

output = spark.createDataFrame(output,['cuisine','topic','topic_words'])

pandas_df = output.toPandas()

pandas_df

csv_file_path = "/content/drive/MyDrive/lda-output-negative.csv"

pandas_df.to_csv(csv_file_path, index=False)

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Extract the topic_words for entries 9 to 12
topic_words_9_to_12 = pandas_df.loc[9:12, 'topic_words']

# Concatenate the words for each entry into separate strings
combined_text_9 = ' '.join(topic_words_9_to_12.iloc[0])
combined_text_10 = ' '.join(topic_words_9_to_12.iloc[1])
combined_text_11 = ' '.join(topic_words_9_to_12.iloc[2])

# Create a custom color function for the word cloud
def red_orange_color_func(word, font_size, position, orientation, random_state=888, **kwargs):
    return 'rgb(255, {}, 0)'.format(int(255.0 * font_size / 100))

# Create word clouds for each entry
wordcloud_9 = WordCloud(width=400, height=200, background_color='white',color_func=red_orange_color_func).generate(combined_text_9)
wordcloud_10 = WordCloud(width=400, height=200, background_color='white',color_func=red_orange_color_func).generate(combined_text_10)
wordcloud_11 = WordCloud(width=400, height=200, background_color='white',color_func=red_orange_color_func).generate(combined_text_11)

# Plot the word clouds in a 1x3 grid
plt.figure(figsize=(12, 4))

plt.subplot(131)
plt.imshow(wordcloud_9, interpolation='bilinear')
plt.axis('off')
plt.title("Japanese - Topic 1\n(Bad Reviews)")

plt.subplot(132)
plt.imshow(wordcloud_10, interpolation='bilinear')
plt.axis('off')
plt.title("Japanese - Topic 2\n(Bad Reviews)")

plt.subplot(133)
plt.imshow(wordcloud_11, interpolation='bilinear')
plt.axis('off')
plt.title("Japanese - Topic 3\n(Bad Reviews)")

plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Extract the topic_words for entries 12 to 15
topic_words_12_to_15 = pandas_df.loc[12:15, 'topic_words']

# Concatenate the words for each entry into separate strings
combined_text_12 = ' '.join(topic_words_12_to_15.iloc[0])
combined_text_13 = ' '.join(topic_words_12_to_15.iloc[1])
combined_text_14 = ' '.join(topic_words_12_to_15.iloc[2])

# Create word clouds for each entry
wordcloud_12 = WordCloud(width=400, height=200, background_color='white',color_func=red_orange_color_func).generate(combined_text_12)
wordcloud_13 = WordCloud(width=400, height=200, background_color='white',color_func=red_orange_color_func).generate(combined_text_13)
wordcloud_14 = WordCloud(width=400, height=200, background_color='white',color_func=red_orange_color_func).generate(combined_text_14)

# Plot the word clouds in a 1x3 grid
plt.figure(figsize=(12, 4))

plt.subplot(131)
plt.imshow(wordcloud_12, interpolation='bilinear')
plt.axis('off')
plt.title("Indian - Topic 1\n(Bad Reviews)")

plt.subplot(132)
plt.imshow(wordcloud_13, interpolation='bilinear')
plt.axis('off')
plt.title("Indian - Topic 2\n(Bad Reviews)")

plt.subplot(133)
plt.imshow(wordcloud_14, interpolation='bilinear')
plt.axis('off')
plt.title("Indian - Topic 3\n(Bad Reviews)")

plt.show()


##### High Score LDA with Word Clouds
!pip install pyspark

from google.colab import drive
drive.mount('/content/drive')

from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
from pyspark import SparkConf

from pyspark.ml.feature import Tokenizer, CountVectorizer
from pyspark.ml.clustering import LDA
from pyspark.ml import Pipeline
import pyspark.sql.functions as sqlf
from pyspark.sql.types import StructType, StructField, StringType, ArrayType
from pyspark.sql import Row

conf = SparkConf().setMaster("local[*]").set("spark.executer.memory", "10g")

sc = SparkContext(conf=conf)
spark = SparkSession(sc).builder.getOrCreate()
spark.sparkContext.setLogLevel("ERROR")

df = spark.read.format("csv").option("header", "true").option("multiline","true").load("/content/drive/MyDrive/yelp-dataset/yelp_review.csv")
# df.printSchema()

df = df.withColumn("review_stars", df["stars"].cast("double"))
df = df.filter(df.review_stars.isin(4.0,5.0))
df = df.drop("stars")
df = df.drop("useful","funny","cool")

# df.show()

yelp_business_df = spark.read.csv('/content/drive/MyDrive/yelp-dataset/yelp_business.csv', header=True, inferSchema=True)
# yelp_business_df.printSchema()
yelp_business_df = yelp_business_df.drop("neighborhood","address","city","postal_code","latitude","longitude")
yelp_business_df = yelp_business_df.withColumnRenamed("stars", "business_stars")
# yelp_business_df.show(60)

yelp_business_df = yelp_business_df.filter(yelp_business_df["is_open"] == 1.0)
# yelp_business_df.show()

food_words = ['Restaurant', 'Restaurants', 'Food', 'Fast Food', 'Bakery', 'Ice Cream & Frozen Yogurt', 'Cafe', 'Soup', 'Salad', 'Coffee', 'Tea', 'Food Truck', 'Pizza', 'Sandwich', 'Bar', 'Buffet', 'Diner', 'Donut', 'Juice Bar', 'Smoothie', 'Deli', 'Bagel', 'Dessert', 'Food Stand', 'Street Vendor', 'Tea Room']
cuisine = ["American", "Italian", "Mexican", "Chinese", "Japanese", "Indian", "French", "Thai", "Mediterranean", "Greek", "Spanish", "Vietnamese", "Korean", "Lebanese", "Brazilian", "Turkish", "Caribbean", "Ethiopian"]

#yelp_business_df.select("categories").show(10,truncate = False)

# tokenized_business_df = yelp_business_df.withColumn("category_tokens", explode(split(col("categories"), ";")))

#  # Filter the DataFrame to include only rows where the category token matches food_words or cuisine
#  filtered_df = tokenized_business_df.filter(
#      (tokenized_business_df["category_tokens"].isin(food_words)) |
#      (tokenized_business_df["category_tokens"].isin(cuisine))
#  )

from pyspark.sql.functions import split, explode, col, collect_list


tokenized_df = yelp_business_df.withColumn("category_tokens", explode(split(col("categories"), ";")))

# Apply the filtering logic to include only rows where category_tokens match food_words or cuisine
filtered_df = tokenized_df.filter(
    (tokenized_df["category_tokens"].isin(food_words)) |
    (tokenized_df["category_tokens"].isin(cuisine))
)

# Group by business_id and aggregate category tokens into a list
grouped_df = filtered_df.groupBy("business_id").agg(collect_list("category_tokens").alias("category_tokens"))

# Join the original DataFrame with the grouped DataFrame on business_id
result_df = yelp_business_df.join(grouped_df, "business_id", "inner")

# result_df.show(30)

reviews_df = df.drop("user_id", "date")
# reviews_df.show()

# result_df.printSchema()

from pyspark.sql.types import DoubleType

# Convert the "business_stars" column to DoubleType
result_df = result_df.withColumn("business_stars", result_df["business_stars"].cast(DoubleType()))

# Filter the DataFrame to include only rows where "business_stars" is above 3.49
above_average_df = result_df.filter(result_df["business_stars"] > 3.49)

# above_average_df.show()

merged_df = above_average_df.join(reviews_df, "business_id", "inner")

# merged_df.show()

"""Part II: Analysis"""

# merged_df2 = merged_df.withColumn("cuisine_list", sqlf.lit(cuisine))
merged_df2 = merged_df.filter(sqlf.arrays_overlap(merged_df['category_tokens'],sqlf.lit(cuisine)))
# merged_df2.show()
merged_df2.persist()

import string
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk import punkt
stopword_list = set(stopwords.words('english'))
stopword_list.update(['food', 'great', 'good', 'place','amazing', 'restaurant', 'back', 'delicious', 'really', 'best', 'love', 'go', 'like', 'get'])

def ProcessText(text):
  tokens = nltk.word_tokenize(text)
  remove_punct = [word.lower() for word in tokens if word.isalpha()]
  remove_stop_words = [word for word in remove_punct if not word in stopword_list]
  return remove_stop_words

from matplotlib import pyplot as plt
from wordcloud import WordCloud, STOPWORDS
import matplotlib.colors as mcolors

output = []
num_topics = 3
iter_num = 10
min_doc_freq = 0.05
max_doc_freq = 0.90

cuisine = ["American", "Italian", "Mexican", "Chinese", "Japanese", "Indian", "French", "Thai", "Mediterranean", "Greek", "Spanish", "Vietnamese", "Korean", "Lebanese", "Brazilian", "Turkish", "Caribbean", "Ethiopian"]
# cuisine = ["Italian", "Mexican", "Chinese"]

for key_cuisine in cuisine:
  # First filter merged_df2 to only the cuisine we care about
  cuisine_df = merged_df2.filter(sqlf.arrays_overlap(merged_df2["category_tokens"], sqlf.lit([key_cuisine])))\
    .select("business_id", "review_count", "review_id", "text")\
    .withColumn("cuisine", sqlf.lit(key_cuisine)
    )

  print(key_cuisine,"rows:",cuisine_df.count())
  if cuisine_df.count() == 0: continue

  # Tokenizing the text of the filtered reviews
  tok_rev = cuisine_df.rdd.map(lambda x: [x['business_id'], x['review_count'], x['review_id'], ProcessText(x['text']), x['cuisine']]).toDF()
  tok_rev = tok_rev.withColumnsRenamed(dict(zip(tok_rev.columns, cuisine_df.columns)))

  # Count vectorizing the text of the reviews
  cv = CountVectorizer(minDF=min_doc_freq, maxDF=max_doc_freq, inputCol='text', outputCol = 'features')
  cv_model = cv.fit(tok_rev)
  cv_rev = cv_model.transform(tok_rev)

  # Initializing the LDA model and fitting it to count vectorized reviews
  lda = LDA(k=num_topics, maxIter=iter_num)
  model = lda.fit(cv_rev)

  # Extracting the words from the topics of the LDA model
  vocab = cv_model.vocabulary
  topics = model.describeTopics()

  topic_groups = topics.rdd.map(lambda row: (key_cuisine, row['topic'], row['termIndices']))\
    .map(lambda idx_list: (idx_list[0], idx_list[1], [vocab[idx] for idx in idx_list[2]]))\
    .collect()

  # Extending the output list
  output.extend(topic_groups)
  print(key_cuisine, 'analysis completed')

spark.createDataFrame(output,['cuisine','topic','topic_words']).show(truncate=False)

spark.createDataFrame(output,['cuisine','topic','topic_words']).collect()

output = spark.createDataFrame(output,['cuisine','topic','topic_words'])

pandas_df = output.toPandas()

pandas_df

csv_file_path = "/content/drive/MyDrive/lda-output-positive.csv"

pandas_df.to_csv(csv_file_path, index=False)

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Extract the topic_words for entries 9 to 12
topic_words_9_to_12 = pandas_df.loc[9:12, 'topic_words']

# Concatenate the words for each entry into separate strings
combined_text_9 = ' '.join(topic_words_9_to_12.iloc[0])
combined_text_10 = ' '.join(topic_words_9_to_12.iloc[1])
combined_text_11 = ' '.join(topic_words_9_to_12.iloc[2])

# Create word clouds for each entry
wordcloud_9 = WordCloud(width=400, height=200, background_color='white').generate(combined_text_9)
wordcloud_10 = WordCloud(width=400, height=200, background_color='white').generate(combined_text_10)
wordcloud_11 = WordCloud(width=400, height=200, background_color='white').generate(combined_text_11)

# Plot the word clouds in a 1x3 grid
plt.figure(figsize=(12, 4))

plt.subplot(131)
plt.imshow(wordcloud_9, interpolation='bilinear')
plt.axis('off')
plt.title("Japanese - Topic 1\n(Good Reviews)")

plt.subplot(132)
plt.imshow(wordcloud_10, interpolation='bilinear')
plt.axis('off')
plt.title("Japanese - Topic 2\n(Good Reviews)")

plt.subplot(133)
plt.imshow(wordcloud_11, interpolation='bilinear')
plt.axis('off')
plt.title("Japanese - Topic 3\n(Good Reviews)")

plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Extract the topic_words for entries 12 to 15
topic_words_12_to_15 = pandas_df.loc[12:15, 'topic_words']

# Concatenate the words for each entry into separate strings
combined_text_12 = ' '.join(topic_words_12_to_15.iloc[0])
combined_text_13 = ' '.join(topic_words_12_to_15.iloc[1])
combined_text_14 = ' '.join(topic_words_12_to_15.iloc[2])

# Create word clouds for each entry
wordcloud_12 = WordCloud(width=400, height=200, background_color='white').generate(combined_text_12)
wordcloud_13 = WordCloud(width=400, height=200, background_color='white').generate(combined_text_13)
wordcloud_14 = WordCloud(width=400, height=200, background_color='white').generate(combined_text_14)

# Plot the word clouds in a 1x3 grid
plt.figure(figsize=(12, 4))

plt.subplot(131)
plt.imshow(wordcloud_12, interpolation='bilinear')
plt.axis('off')
plt.title("Indian - Topic 1\n(Good Reviews)")

plt.subplot(132)
plt.imshow(wordcloud_13, interpolation='bilinear')
plt.axis('off')
plt.title("Indian - Topic 2\n(Good Reviews)")

plt.subplot(133)
plt.imshow(wordcloud_14, interpolation='bilinear')
plt.axis('off')
plt.title("Indian - Topic 3\n(Good Reviews)")

plt.show()

